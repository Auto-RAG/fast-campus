{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Multimodal Parsing with LlamaParse",
   "id": "e623b1e895bc124e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Set Environment variables",
   "id": "4ad7445679543f68"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T06:54:46.824838Z",
     "start_time": "2024-12-21T06:54:46.812560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ],
   "id": "b95f09176fc88c52",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T06:54:59.220927Z",
     "start_time": "2024-12-21T06:54:59.216494Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ],
   "id": "dbd1d818838d0531",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LlamaParse로 돌려보기",
   "id": "d50f26f64112043d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T06:55:12.349972Z",
     "start_time": "2024-12-21T06:55:12.344052Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from glob import glob\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "root_dir = os.path.join(current_dir, '..', '..')\n",
    "data_dir = os.path.join(root_dir, 'data', 'parse_multimodal')\n",
    "glob_path = os.path.join(data_dir, '*')\n",
    "\n",
    "pdf_path = glob(glob_path)\n",
    "pdf_path"
   ],
   "id": "4d31295f4bc26c4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/kimbwook/PycharmProjects/fast-campus/notebook/파싱/../../data/parse_multimodal/Llama_image_text.pdf',\n",
       " '/Users/kimbwook/PycharmProjects/fast-campus/notebook/파싱/../../data/parse_multimodal/Llama_image.pdf']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Model list \n",
    "\n",
    "https://docs.cloud.llamaindex.ai/llamaparse/features/multimodal\n",
    "\n",
    "- openai-gpt4o       \n",
    "- openai-gpt-4o-mini\n",
    "- anthropic-sonnet-3.5\n",
    "- gemini-1.5-flash\n",
    "- gemini-1.5-pro"
   ],
   "id": "75e6bf5baf4b0fe5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T06:59:36.030782Z",
     "start_time": "2024-12-21T06:59:33.833736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_parse import LlamaParse\n",
    "\n",
    "parse_instance = LlamaParse(\n",
    "    result_type=\"markdown\", \n",
    "    language=\"ko\", \n",
    "    use_vendor_multimodal_model=True,\n",
    "    vendor_multimodal_model=\"openai-gpt-4o-mini\",\n",
    "    # vendor_multimodal_api_key=\"sk-111\",\n",
    ")"
   ],
   "id": "3e9984c459cec518",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T06:59:59.469717Z",
     "start_time": "2024-12-21T06:59:40.643156Z"
    }
   },
   "cell_type": "code",
   "source": "parse_instance.load_data(pdf_path)",
   "id": "185fd0d23b9ffc8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing files: 100%|██████████| 2/2 [00:18<00:00,  9.40s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id_='f72d5d5b-b4c8-4359-bdc5-61cb319723de', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Introducing Llama 3.1: Our most capable models to date\\n\\nAs our largest model yet, training Llama 3.1 405B on over 16 trillion tokens was a major challenge. To enable training runs at this scale and achieve the results we have in a reasonable amount of time, we significantly optimized our full training stack and pushed our model training to over 16 thousand H100 GPUs, making the 405B the first Llama model trained at this scale.\\n\\n```\\n[INPUT]\\nText tokens\\n\\n[Token embeddings]\\n\\n[Self-attention]\\n[Feedforward network]\\n...\\n\\n[Self-attention]\\n[Feedforward network]\\n\\n[OUTPUT]\\nText token\\n\\nAUTOREGRESSIVE DECODING\\n```\\n\\nTo address this, we made design choices that focus on keeping the model development process scalable and straightforward.\\n\\n- We opted for a standard decoder-only transformer model architecture with minor adaptations rather than a mixture-of-experts model to maximize training stability.\\n- We adopted an iterative post-training procedure, where each round uses supervised fine-tuning and direct preference optimization. This enabled us to create the highest quality synthetic data for each round and improve each capability’s performance.\\n\\nCompared to previous versions of Llama, we improved both the quantity and quality of the data we use for pre- and post-training. These improvements include the development of more careful pre-processing and curation pipelines for pre-training data, the development of more rigorous quality assurance, and filtering approaches for post-training data.\\n\\nAs expected per scaling laws for language models, our new flagship model outperforms smaller models trained using the same procedure. We also used the 405B parameter model to improve the post-training quality of our smaller models.\\n\\nTo support large-scale production inference for a model at the scale of the 405B, we quantized our models from 16-bit (BF16) to 8-bit (FP8) numerics, effectively lowering the compute requirements needed and allowing the model to run within a single server node.\\n\\n## Instruction and chat fine-tuning\\n\\nWith Llama 3.1 405B, we strove to improve the helpfulness, quality, and detailed instruction-following capability of the model in response to user instructions while.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='63a1aadb-6cb0-45e0-87b7-a6cada062e3f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Introducing Llama 3.1: Our most capable models to date\\n\\n## Meta\\n\\n| Category        | Benchmark          | Llama 3.1 8B | Gemma 2 9B IT | Mistral 7B Instruct | Llama 3.1 70B | Mixtral 8x228 Instruct | GPT 3.5 Turbo |\\n|-----------------|--------------------|--------------|---------------|---------------------|--------------|------------------------|---------------|\\n| General         | MMLU (0-shot, CoT) | 73.0         | 72.3          | 60.5                | 86.0         | 79.9                   | 69.8          |\\n|                 | MMLU PRO (5-shot, CoT) | 48.3     | -             | 36.9                | 66.4         | 56.3                   | 49.2          |\\n|                 | IFEval             | 80.4         | 73.6          | 57.6                | 87.5         | 72.7                   | 69.9          |\\n| Code            | HumanEval (0-shot) | 72.6         | 54.3          | 40.2                | 80.5         | 75.6                   | 68.0          |\\n|                 | MBPP EvalPlus (5-shot) | 72.8     | 71.7          | 49.5                | 86.0         | 78.6                   | 82.0          |\\n| Math            | GSM8K (8-shot, CoT) | 84.5        | 76.7          | 53.2                | 95.1         | 88.2                   | 81.6          |\\n|                 | MATH (0-shot, CoT) | 51.9         | 44.3          | 13.0                | 68.0         | 54.1                   | 43.1          |\\n| Reasoning       | ARC Challenge (0-shot) | 83.4     | 87.6          | 74.2                | 94.8         | 88.7                   | 83.7          |\\n|                 | GPOA (0-shot, CoT) | 32.8         | -             | 28.8                | 46.7         | 33.3                   | 30.8          |\\n| Tool use        | BFCL               | 76.1         | -             | 60.4                | 84.8         | -                      | 85.9          |\\n|                 | Nexus              | 38.5         | 30.0          | 24.7                | 56.7         | 48.5                   | 37.2          |\\n| Long context    | ZeroSCROLLS/QUALITY | 81.0       | -             | -                   | 90.5         | -                      | -             |\\n|                 | InfiniteBench/Inf.MC | 65.1      | -             | -                   | 78.2         | -                      | -             |\\n|                 | NINJ/Multi-needle  | 98.8         | -             | -                   | 97.5         | -                      | -             |\\n| Multilingual    | Multilingual MGSM (0-shot) | 68.9 | 53.2          | 29.9                | 86.9         | 71.1                   | 51.4          |\\n\\n## Llama 3.1 405B Human Evaluation\\n\\n| Comparison                          | Win  | Tie  | Loss |\\n|-------------------------------------|------|------|------|\\n| Llama 3.1 405B vs GPT-4-0125-Preview | 23.3% | 52.2% | 24.5% |\\n| Llama 3.1 405B vs GPT-4o            | 19.1% | 51.7% | 29.2% |\\n| Llama 3.1 405B vs Claude 3.5 Sonnet | 24.9% | 50.8% | 24.2% |\\n\\nhttps://ai.meta.com/blog/meta-llama-3-1/', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## AutoRAG로 돌려보기",
   "id": "ef461895c68a96c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T07:00:46.241621Z",
     "start_time": "2024-12-21T07:00:43.444087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from autorag.parser import Parser\n",
    "\n",
    "project_dir = os.path.join(root_dir, 'autorag_project', 'parse', 'multimodal')\n",
    "parser = Parser(data_path_glob=glob_path, project_dir=project_dir)"
   ],
   "id": "bce30fe008aab5c6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[2;36m[12/21/24 16:00:45]\u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m \u001B[1m[\u001B[0m__init__.py:\u001B[1;36m100\u001B[0m\u001B[1m]\u001B[0m >> You are using API version of AutoRAG.To use local \u001B]8;id=691440;file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/autorag/__init__.py\u001B\\\u001B[2m__init__.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=644297;file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/autorag/__init__.py#100\u001B\\\u001B[2m100\u001B[0m\u001B]8;;\u001B\\\n",
       "\u001B[2;36m                    \u001B[0m         version, run pip install \u001B[32m'AutoRAG\u001B[0m\u001B[32m[\u001B[0m\u001B[32mgpu\u001B[0m\u001B[32m]\u001B[0m\u001B[32m'\u001B[0m                                \u001B[2m               \u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/21/24 16:00:45] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-weight: bold\">[</span>__init__.py:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span><span style=\"font-weight: bold\">]</span> &gt;&gt; You are using API version of AutoRAG.To use local <a href=\"file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/autorag/__init__.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">__init__.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/autorag/__init__.py#100\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">100</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         version, run pip install <span style=\"color: #008000; text-decoration-color: #008000\">'AutoRAG[gpu]'</span>                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[2;36m                   \u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m \u001B[1m[\u001B[0m__init__.py:\u001B[1;36m128\u001B[0m\u001B[1m]\u001B[0m >> You are using API version of AutoRAG.To use local \u001B]8;id=738256;file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/autorag/__init__.py\u001B\\\u001B[2m__init__.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=844296;file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/autorag/__init__.py#128\u001B\\\u001B[2m128\u001B[0m\u001B]8;;\u001B\\\n",
       "\u001B[2;36m                    \u001B[0m         version, run pip install \u001B[32m'AutoRAG\u001B[0m\u001B[32m[\u001B[0m\u001B[32mgpu\u001B[0m\u001B[32m]\u001B[0m\u001B[32m'\u001B[0m                                \u001B[2m               \u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-weight: bold\">[</span>__init__.py:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span><span style=\"font-weight: bold\">]</span> &gt;&gt; You are using API version of AutoRAG.To use local <a href=\"file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/autorag/__init__.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">__init__.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/autorag/__init__.py#128\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">128</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         version, run pip install <span style=\"color: #008000; text-decoration-color: #008000\">'AutoRAG[gpu]'</span>                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T07:02:38.037073Z",
     "start_time": "2024-12-21T07:02:11.747467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "yaml_path = os.path.join(root_dir, 'config', 'parse', 'multimodal.yaml')\n",
    "parser.start_parsing(yaml_path, all_files=True)"
   ],
   "id": "363c70c719a0a4de",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[2;36m[12/21/24 16:02:11]\u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m \u001B[1m[\u001B[0mparser.py:\u001B[1;36m29\u001B[0m\u001B[1m]\u001B[0m >> Parsing Start\u001B[33m...\u001B[0m                                        \u001B]8;id=981886;file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/autorag/parser.py\u001B\\\u001B[2mparser.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=810944;file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/autorag/parser.py#29\u001B\\\u001B[2m29\u001B[0m\u001B]8;;\u001B\\\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/21/24 16:02:11] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-weight: bold\">[</span>parser.py:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29</span><span style=\"font-weight: bold\">]</span> &gt;&gt; Parsing Start<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                        <a href=\"file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/autorag/parser.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">parser.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/autorag/parser.py#29\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">29</span></a>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[2;36m                   \u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m \u001B[1m[\u001B[0mbase.py:\u001B[1;36m23\u001B[0m\u001B[1m]\u001B[0m >> Running parser - llama_parse module\u001B[33m...\u001B[0m                      \u001B]8;id=878370;file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/autorag/data/parse/base.py\u001B\\\u001B[2mbase.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=220598;file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/autorag/data/parse/base.py#23\u001B\\\u001B[2m23\u001B[0m\u001B]8;;\u001B\\\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-weight: bold\">[</span>base.py:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23</span><span style=\"font-weight: bold\">]</span> &gt;&gt; Running parser - llama_parse module<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                      <a href=\"file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/autorag/data/parse/base.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">base.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/autorag/data/parse/base.py#23\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">23</span></a>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[2;36m[12/21/24 16:02:13]\u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m \u001B[1m[\u001B[0m_client.py:\u001B[1;36m1773\u001B[0m\u001B[1m]\u001B[0m >> HTTP Request: \u001B[1;33mPOST\u001B[0m                                \u001B]8;id=976705;file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py\u001B\\\u001B[2m_client.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=787396;file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py#1773\u001B\\\u001B[2m1773\u001B[0m\u001B]8;;\u001B\\\n",
       "\u001B[2;36m                    \u001B[0m         \u001B[4;94mhttps://api.cloud.llamaindex.ai/api/parsing/upload\u001B[0m \u001B[32m\"HTTP/1.1 200 OK\"\u001B[0m   \u001B[2m               \u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/21/24 16:02:13] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-weight: bold\">[</span>_client.py:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1773</span><span style=\"font-weight: bold\">]</span> &gt;&gt; HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span>                                <a href=\"file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py#1773\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1773</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://api.cloud.llamaindex.ai/api/parsing/upload</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id f27f4c2d-4638-457f-8f73-aaf96206f482\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001B[2;36m[12/21/24 16:02:15]\u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m \u001B[1m[\u001B[0m_client.py:\u001B[1;36m1773\u001B[0m\u001B[1m]\u001B[0m >> HTTP Request: \u001B[1;33mGET\u001B[0m                                 \u001B]8;id=382747;file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py\u001B\\\u001B[2m_client.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=401272;file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py#1773\u001B\\\u001B[2m1773\u001B[0m\u001B]8;;\u001B\\\n",
       "\u001B[2;36m                    \u001B[0m         \u001B[4;94mhttps://api.cloud.llamaindex.ai/api/parsing/job/f27f4c2d-4638-457f-8f7\u001B[0m \u001B[2m               \u001B[0m\n",
       "\u001B[2;36m                    \u001B[0m         \u001B[4;94m3-aaf96206f482\u001B[0m \u001B[32m\"HTTP/1.1 200 OK\"\u001B[0m                                       \u001B[2m               \u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/21/24 16:02:15] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-weight: bold\">[</span>_client.py:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1773</span><span style=\"font-weight: bold\">]</span> &gt;&gt; HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">GET</span>                                 <a href=\"file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py#1773\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1773</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://api.cloud.llamaindex.ai/api/parsing/job/f27f4c2d-4638-457f-8f7</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">3-aaf96206f482</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>                                       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[2;36m[12/21/24 16:02:17]\u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m \u001B[1m[\u001B[0m_client.py:\u001B[1;36m1773\u001B[0m\u001B[1m]\u001B[0m >> HTTP Request: \u001B[1;33mPOST\u001B[0m                                \u001B]8;id=410198;file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py\u001B\\\u001B[2m_client.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=674973;file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py#1773\u001B\\\u001B[2m1773\u001B[0m\u001B]8;;\u001B\\\n",
       "\u001B[2;36m                    \u001B[0m         \u001B[4;94mhttps://api.cloud.llamaindex.ai/api/parsing/upload\u001B[0m \u001B[32m\"HTTP/1.1 200 OK\"\u001B[0m   \u001B[2m               \u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/21/24 16:02:17] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-weight: bold\">[</span>_client.py:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1773</span><span style=\"font-weight: bold\">]</span> &gt;&gt; HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span>                                <a href=\"file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py#1773\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1773</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://api.cloud.llamaindex.ai/api/parsing/upload</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 9103a76b-742d-4f7c-9401-d72dec892d43\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001B[2;36m[12/21/24 16:02:18]\u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m \u001B[1m[\u001B[0m_client.py:\u001B[1;36m1773\u001B[0m\u001B[1m]\u001B[0m >> HTTP Request: \u001B[1;33mGET\u001B[0m                                 \u001B]8;id=380403;file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py\u001B\\\u001B[2m_client.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=418188;file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py#1773\u001B\\\u001B[2m1773\u001B[0m\u001B]8;;\u001B\\\n",
       "\u001B[2;36m                    \u001B[0m         \u001B[4;94mhttps://api.cloud.llamaindex.ai/api/parsing/job/f27f4c2d-4638-457f-8f7\u001B[0m \u001B[2m               \u001B[0m\n",
       "\u001B[2;36m                    \u001B[0m         \u001B[4;94m3-aaf96206f482\u001B[0m \u001B[32m\"HTTP/1.1 200 OK\"\u001B[0m                                       \u001B[2m               \u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/21/24 16:02:18] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-weight: bold\">[</span>_client.py:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1773</span><span style=\"font-weight: bold\">]</span> &gt;&gt; HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">GET</span>                                 <a href=\"file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py#1773\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1773</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://api.cloud.llamaindex.ai/api/parsing/job/f27f4c2d-4638-457f-8f7</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">3-aaf96206f482</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>                                       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[2;36m[12/21/24 16:02:19]\u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m \u001B[1m[\u001B[0m_client.py:\u001B[1;36m1773\u001B[0m\u001B[1m]\u001B[0m >> HTTP Request: \u001B[1;33mGET\u001B[0m                                 \u001B]8;id=277365;file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py\u001B\\\u001B[2m_client.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=854340;file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py#1773\u001B\\\u001B[2m1773\u001B[0m\u001B]8;;\u001B\\\n",
       "\u001B[2;36m                    \u001B[0m         \u001B[4;94mhttps://api.cloud.llamaindex.ai/api/parsing/job/9103a76b-742d-4f7c-940\u001B[0m \u001B[2m               \u001B[0m\n",
       "\u001B[2;36m                    \u001B[0m         \u001B[4;94m1-d72dec892d43\u001B[0m \u001B[32m\"HTTP/1.1 200 OK\"\u001B[0m                                       \u001B[2m               \u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/21/24 16:02:19] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-weight: bold\">[</span>_client.py:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1773</span><span style=\"font-weight: bold\">]</span> &gt;&gt; HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">GET</span>                                 <a href=\"file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py#1773\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1773</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://api.cloud.llamaindex.ai/api/parsing/job/9103a76b-742d-4f7c-940</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">1-d72dec892d43</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>                                       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[2;36m[12/21/24 16:02:20]\u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m \u001B[1m[\u001B[0m_client.py:\u001B[1;36m1773\u001B[0m\u001B[1m]\u001B[0m >> HTTP Request: \u001B[1;33mGET\u001B[0m                                 \u001B]8;id=63306;file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py\u001B\\\u001B[2m_client.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=682826;file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py#1773\u001B\\\u001B[2m1773\u001B[0m\u001B]8;;\u001B\\\n",
       "\u001B[2;36m                    \u001B[0m         \u001B[4;94mhttps://api.cloud.llamaindex.ai/api/parsing/job/f27f4c2d-4638-457f-8f7\u001B[0m \u001B[2m               \u001B[0m\n",
       "\u001B[2;36m                    \u001B[0m         \u001B[4;94m3-aaf96206f482\u001B[0m \u001B[32m\"HTTP/1.1 200 OK\"\u001B[0m                                       \u001B[2m               \u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/21/24 16:02:20] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-weight: bold\">[</span>_client.py:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1773</span><span style=\"font-weight: bold\">]</span> &gt;&gt; HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">GET</span>                                 <a href=\"file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py#1773\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1773</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://api.cloud.llamaindex.ai/api/parsing/job/f27f4c2d-4638-457f-8f7</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">3-aaf96206f482</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>                                       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[2;36m[12/21/24 16:02:23]\u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m \u001B[1m[\u001B[0m_client.py:\u001B[1;36m1773\u001B[0m\u001B[1m]\u001B[0m >> HTTP Request: \u001B[1;33mGET\u001B[0m                                 \u001B]8;id=860443;file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py\u001B\\\u001B[2m_client.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=248113;file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py#1773\u001B\\\u001B[2m1773\u001B[0m\u001B]8;;\u001B\\\n",
       "\u001B[2;36m                    \u001B[0m         \u001B[4;94mhttps://api.cloud.llamaindex.ai/api/parsing/job/9103a76b-742d-4f7c-940\u001B[0m \u001B[2m               \u001B[0m\n",
       "\u001B[2;36m                    \u001B[0m         \u001B[4;94m1-d72dec892d43\u001B[0m \u001B[32m\"HTTP/1.1 200 OK\"\u001B[0m                                       \u001B[2m               \u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/21/24 16:02:23] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-weight: bold\">[</span>_client.py:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1773</span><span style=\"font-weight: bold\">]</span> &gt;&gt; HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">GET</span>                                 <a href=\"file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py#1773\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1773</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://api.cloud.llamaindex.ai/api/parsing/job/9103a76b-742d-4f7c-940</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">1-d72dec892d43</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>                                       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[2;36m                   \u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m \u001B[1m[\u001B[0m_client.py:\u001B[1;36m1773\u001B[0m\u001B[1m]\u001B[0m >> HTTP Request: \u001B[1;33mGET\u001B[0m                                 \u001B]8;id=970553;file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py\u001B\\\u001B[2m_client.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=413367;file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py#1773\u001B\\\u001B[2m1773\u001B[0m\u001B]8;;\u001B\\\n",
       "\u001B[2;36m                    \u001B[0m         \u001B[4;94mhttps://api.cloud.llamaindex.ai/api/parsing/job/f27f4c2d-4638-457f-8f7\u001B[0m \u001B[2m               \u001B[0m\n",
       "\u001B[2;36m                    \u001B[0m         \u001B[4;94m3-aaf96206f482\u001B[0m \u001B[32m\"HTTP/1.1 200 OK\"\u001B[0m                                       \u001B[2m               \u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-weight: bold\">[</span>_client.py:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1773</span><span style=\"font-weight: bold\">]</span> &gt;&gt; HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">GET</span>                                 <a href=\"file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py#1773\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1773</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://api.cloud.llamaindex.ai/api/parsing/job/f27f4c2d-4638-457f-8f7</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">3-aaf96206f482</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>                                       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[2;36m[12/21/24 16:02:24]\u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m \u001B[1m[\u001B[0m_client.py:\u001B[1;36m1773\u001B[0m\u001B[1m]\u001B[0m >> HTTP Request: \u001B[1;33mGET\u001B[0m                                 \u001B]8;id=558312;file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py\u001B\\\u001B[2m_client.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=125803;file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py#1773\u001B\\\u001B[2m1773\u001B[0m\u001B]8;;\u001B\\\n",
       "\u001B[2;36m                    \u001B[0m         \u001B[4;94mhttps://api.cloud.llamaindex.ai/api/parsing/job/f27f4c2d-4638-457f-8f7\u001B[0m \u001B[2m               \u001B[0m\n",
       "\u001B[2;36m                    \u001B[0m         \u001B[4;94m3-aaf96206f482/result/markdown\u001B[0m \u001B[32m\"HTTP/1.1 200 OK\"\u001B[0m                       \u001B[2m               \u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/21/24 16:02:24] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-weight: bold\">[</span>_client.py:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1773</span><span style=\"font-weight: bold\">]</span> &gt;&gt; HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">GET</span>                                 <a href=\"file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py#1773\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1773</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://api.cloud.llamaindex.ai/api/parsing/job/f27f4c2d-4638-457f-8f7</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">3-aaf96206f482/result/markdown</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>                       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[2;36m[12/21/24 16:02:26]\u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m \u001B[1m[\u001B[0m_client.py:\u001B[1;36m1773\u001B[0m\u001B[1m]\u001B[0m >> HTTP Request: \u001B[1;33mGET\u001B[0m                                 \u001B]8;id=230279;file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py\u001B\\\u001B[2m_client.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=894638;file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py#1773\u001B\\\u001B[2m1773\u001B[0m\u001B]8;;\u001B\\\n",
       "\u001B[2;36m                    \u001B[0m         \u001B[4;94mhttps://api.cloud.llamaindex.ai/api/parsing/job/9103a76b-742d-4f7c-940\u001B[0m \u001B[2m               \u001B[0m\n",
       "\u001B[2;36m                    \u001B[0m         \u001B[4;94m1-d72dec892d43\u001B[0m \u001B[32m\"HTTP/1.1 200 OK\"\u001B[0m                                       \u001B[2m               \u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/21/24 16:02:26] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-weight: bold\">[</span>_client.py:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1773</span><span style=\"font-weight: bold\">]</span> &gt;&gt; HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">GET</span>                                 <a href=\"file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py#1773\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1773</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://api.cloud.llamaindex.ai/api/parsing/job/9103a76b-742d-4f7c-940</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">1-d72dec892d43</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>                                       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[2;36m[12/21/24 16:02:28]\u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m \u001B[1m[\u001B[0m_client.py:\u001B[1;36m1773\u001B[0m\u001B[1m]\u001B[0m >> HTTP Request: \u001B[1;33mGET\u001B[0m                                 \u001B]8;id=717444;file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py\u001B\\\u001B[2m_client.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=966100;file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py#1773\u001B\\\u001B[2m1773\u001B[0m\u001B]8;;\u001B\\\n",
       "\u001B[2;36m                    \u001B[0m         \u001B[4;94mhttps://api.cloud.llamaindex.ai/api/parsing/job/9103a76b-742d-4f7c-940\u001B[0m \u001B[2m               \u001B[0m\n",
       "\u001B[2;36m                    \u001B[0m         \u001B[4;94m1-d72dec892d43\u001B[0m \u001B[32m\"HTTP/1.1 200 OK\"\u001B[0m                                       \u001B[2m               \u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/21/24 16:02:28] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-weight: bold\">[</span>_client.py:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1773</span><span style=\"font-weight: bold\">]</span> &gt;&gt; HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">GET</span>                                 <a href=\"file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py#1773\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1773</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://api.cloud.llamaindex.ai/api/parsing/job/9103a76b-742d-4f7c-940</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">1-d72dec892d43</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>                                       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[2;36m[12/21/24 16:02:31]\u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m \u001B[1m[\u001B[0m_client.py:\u001B[1;36m1773\u001B[0m\u001B[1m]\u001B[0m >> HTTP Request: \u001B[1;33mGET\u001B[0m                                 \u001B]8;id=544311;file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py\u001B\\\u001B[2m_client.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=169810;file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py#1773\u001B\\\u001B[2m1773\u001B[0m\u001B]8;;\u001B\\\n",
       "\u001B[2;36m                    \u001B[0m         \u001B[4;94mhttps://api.cloud.llamaindex.ai/api/parsing/job/9103a76b-742d-4f7c-940\u001B[0m \u001B[2m               \u001B[0m\n",
       "\u001B[2;36m                    \u001B[0m         \u001B[4;94m1-d72dec892d43\u001B[0m \u001B[32m\"HTTP/1.1 200 OK\"\u001B[0m                                       \u001B[2m               \u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/21/24 16:02:31] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-weight: bold\">[</span>_client.py:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1773</span><span style=\"font-weight: bold\">]</span> &gt;&gt; HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">GET</span>                                 <a href=\"file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py#1773\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1773</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://api.cloud.llamaindex.ai/api/parsing/job/9103a76b-742d-4f7c-940</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">1-d72dec892d43</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>                                       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[2;36m[12/21/24 16:02:34]\u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m \u001B[1m[\u001B[0m_client.py:\u001B[1;36m1773\u001B[0m\u001B[1m]\u001B[0m >> HTTP Request: \u001B[1;33mGET\u001B[0m                                 \u001B]8;id=94808;file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py\u001B\\\u001B[2m_client.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=848941;file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py#1773\u001B\\\u001B[2m1773\u001B[0m\u001B]8;;\u001B\\\n",
       "\u001B[2;36m                    \u001B[0m         \u001B[4;94mhttps://api.cloud.llamaindex.ai/api/parsing/job/9103a76b-742d-4f7c-940\u001B[0m \u001B[2m               \u001B[0m\n",
       "\u001B[2;36m                    \u001B[0m         \u001B[4;94m1-d72dec892d43\u001B[0m \u001B[32m\"HTTP/1.1 200 OK\"\u001B[0m                                       \u001B[2m               \u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/21/24 16:02:34] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-weight: bold\">[</span>_client.py:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1773</span><span style=\"font-weight: bold\">]</span> &gt;&gt; HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">GET</span>                                 <a href=\"file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py#1773\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1773</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://api.cloud.llamaindex.ai/api/parsing/job/9103a76b-742d-4f7c-940</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">1-d72dec892d43</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>                                       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[2;36m[12/21/24 16:02:37]\u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m \u001B[1m[\u001B[0m_client.py:\u001B[1;36m1773\u001B[0m\u001B[1m]\u001B[0m >> HTTP Request: \u001B[1;33mGET\u001B[0m                                 \u001B]8;id=732822;file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py\u001B\\\u001B[2m_client.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=50811;file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py#1773\u001B\\\u001B[2m1773\u001B[0m\u001B]8;;\u001B\\\n",
       "\u001B[2;36m                    \u001B[0m         \u001B[4;94mhttps://api.cloud.llamaindex.ai/api/parsing/job/9103a76b-742d-4f7c-940\u001B[0m \u001B[2m               \u001B[0m\n",
       "\u001B[2;36m                    \u001B[0m         \u001B[4;94m1-d72dec892d43\u001B[0m \u001B[32m\"HTTP/1.1 200 OK\"\u001B[0m                                       \u001B[2m               \u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/21/24 16:02:37] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-weight: bold\">[</span>_client.py:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1773</span><span style=\"font-weight: bold\">]</span> &gt;&gt; HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">GET</span>                                 <a href=\"file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py#1773\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1773</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://api.cloud.llamaindex.ai/api/parsing/job/9103a76b-742d-4f7c-940</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">1-d72dec892d43</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>                                       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[2;36m                   \u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m \u001B[1m[\u001B[0m_client.py:\u001B[1;36m1773\u001B[0m\u001B[1m]\u001B[0m >> HTTP Request: \u001B[1;33mGET\u001B[0m                                 \u001B]8;id=417974;file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py\u001B\\\u001B[2m_client.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=22906;file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py#1773\u001B\\\u001B[2m1773\u001B[0m\u001B]8;;\u001B\\\n",
       "\u001B[2;36m                    \u001B[0m         \u001B[4;94mhttps://api.cloud.llamaindex.ai/api/parsing/job/9103a76b-742d-4f7c-940\u001B[0m \u001B[2m               \u001B[0m\n",
       "\u001B[2;36m                    \u001B[0m         \u001B[4;94m1-d72dec892d43/result/markdown\u001B[0m \u001B[32m\"HTTP/1.1 200 OK\"\u001B[0m                       \u001B[2m               \u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-weight: bold\">[</span>_client.py:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1773</span><span style=\"font-weight: bold\">]</span> &gt;&gt; HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">GET</span>                                 <a href=\"file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/httpx/_client.py#1773\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1773</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://api.cloud.llamaindex.ai/api/parsing/job/9103a76b-742d-4f7c-940</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">1-d72dec892d43/result/markdown</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>                       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[2;36m[12/21/24 16:02:38]\u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m \u001B[1m[\u001B[0mparser.py:\u001B[1;36m37\u001B[0m\u001B[1m]\u001B[0m >> Parsing Done!                                           \u001B]8;id=311275;file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/autorag/parser.py\u001B\\\u001B[2mparser.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=201532;file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/autorag/parser.py#37\u001B\\\u001B[2m37\u001B[0m\u001B]8;;\u001B\\\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/21/24 16:02:38] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-weight: bold\">[</span>parser.py:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">37</span><span style=\"font-weight: bold\">]</span> &gt;&gt; Parsing Done!                                           <a href=\"file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/autorag/parser.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">parser.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/kimbwook/PycharmProjects/fast-campus/venv/lib/python3.10/site-packages/autorag/parser.py#37\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">37</span></a>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 결과 확인",
   "id": "cf3e21535f951dc6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "https://markdownlivepreview.com/ 여기 사이트에서 더 예쁘게 마크다운을 확인할 수 있다.",
   "id": "ceb0cc3b28e27122"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T07:03:10.504700Z",
     "start_time": "2024-12-21T07:03:10.501801Z"
    }
   },
   "cell_type": "code",
   "source": "import pandas as pd",
   "id": "b005afcdab04b0bd",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T07:03:13.587783Z",
     "start_time": "2024-12-21T07:03:13.556481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result_path = os.path.join(project_dir, 'parsed_result.parquet')\n",
    "multimodal_result = pd.read_parquet(result_path)\n",
    "multimodal_result"
   ],
   "id": "c0d15f08b93bcb25",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                               texts  \\\n",
       "0  # Introducing Llama 3.1: Our most capable mode...   \n",
       "1  # Introducing Llama 3.1: Our most capable mode...   \n",
       "\n",
       "                                                path  page  \\\n",
       "0  /Users/kimbwook/PycharmProjects/fast-campus/no...     1   \n",
       "1  /Users/kimbwook/PycharmProjects/fast-campus/no...     1   \n",
       "\n",
       "  last_modified_datetime  \n",
       "0             2024-12-11  \n",
       "1             2024-12-11  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>path</th>\n",
       "      <th>page</th>\n",
       "      <th>last_modified_datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td># Introducing Llama 3.1: Our most capable mode...</td>\n",
       "      <td>/Users/kimbwook/PycharmProjects/fast-campus/no...</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-12-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td># Introducing Llama 3.1: Our most capable mode...</td>\n",
       "      <td>/Users/kimbwook/PycharmProjects/fast-campus/no...</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-12-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T07:03:35.890057Z",
     "start_time": "2024-12-21T07:03:35.886551Z"
    }
   },
   "cell_type": "code",
   "source": "texts = multimodal_result[\"texts\"].tolist()",
   "id": "c9c22580d75e2e15",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T07:03:38.409881Z",
     "start_time": "2024-12-21T07:03:38.406514Z"
    }
   },
   "cell_type": "code",
   "source": "print(texts[0])",
   "id": "f23d7785d12111eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Introducing Llama 3.1: Our most capable models to date\n",
      "\n",
      "As our largest model yet, training Llama 3.1 on over 16 trillion tokens was a major challenge. To enable training runs at this scale and achieve the results we have in a reasonable amount of time, we significantly optimized our full training stack and pushed our model training to over 16 thousand H100 GPUs, making the 405B the first Llama model trained at this scale.\n",
      "\n",
      "## Model Architecture\n",
      "\n",
      "- **INPUT**: Text tokens\n",
      "- **Token embeddings**\n",
      "- **Self-attention**\n",
      "- **Feedforward network**\n",
      "- **...**\n",
      "- **Self-attention**\n",
      "- **Feedforward network**\n",
      "- **OUTPUT**: Text token\n",
      "- **AUTOREGRESSIVE DECODING**\n",
      "\n",
      "To address this, we made design choices that focus on keeping the model development process scalable and straightforward.\n",
      "\n",
      "- We opted for a standard decoder-only transformer model architecture with minor adaptations rather than a mixture-of-experts model to maximize training stability.\n",
      "- We adopted an iterative post-training procedure, where each round uses supervised fine-tuning and direct preference optimization. This enabled us to create the highest quality synthetic data for each round and improve each capability’s performance.\n",
      "\n",
      "Compared to previous versions of Llama, we improved both the quantity and quality of the data we use for pre- and post-training. These improvements include the development of more careful pre-processing and curation pipelines for pre-training data, the development of more rigorous quality assurance, and filtering approaches for post-training data.\n",
      "\n",
      "As expected per scaling laws for language models, our new flagship model outperforms smaller models trained using the same procedure. We also used the 405B parameter model to improve the post-training quality of our smaller models.\n",
      "\n",
      "## Instruction and Chat Fine-tuning\n",
      "\n",
      "With Llama 3.1 405B, we strove to improve the helpfulness, quality, and detailed instruction-following capability of the model in response to user instructions while\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T05:56:13.911865Z",
     "start_time": "2024-12-20T05:56:13.909062Z"
    }
   },
   "cell_type": "code",
   "source": "print(texts[1])",
   "id": "88e467390432c01c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Introducing Llama 3.1: Our most capable models to date\n",
      "\n",
      "| Category        | Benchmark                     | Llama 3.1 88B | Gemma 2 9B IT | Mistral 7B Instruct | Llama 3.1 70B | Mixtral 8x228 Instruct | GPT 3.5 Turbo |\n",
      "|-----------------|-------------------------------|----------------|----------------|---------------------|----------------|------------------------|----------------|\n",
      "| General         | MMLU (0-shot, CoT)           | 73.0           | 72.3           | 60.5                | 86.0           | 79.9                   | 69.8           |\n",
      "|                 | MMLU PRO (5-shot, CoT)       | 48.3           | 36.9           | 36.9                | 66.4           | 56.3                   | 49.2           |\n",
      "|                 | IFEval                        | 80.4           | 73.6           | 57.6                | 87.5           | 72.7                   | 69.9           |\n",
      "| Code            | HumanEval (0-shot)           | 72.6           | 54.3           | 40.2                | 80.5           | 76.0                   | 68.0           |\n",
      "|                 | MBPP EvalPlus (0-shot)       | 72.8           | 71.7           | 49.5                | 86.0           | 78.6                   | 82.0           |\n",
      "| Math            | GSMKB (0-shot, CoT)          | 84.5           | 76.7           | 53.2                | 95.1           | 88.2                   | 81.6           |\n",
      "|                 | MATH (0-shot, CoT)           | 51.9           | 44.3           | 13.0                | 68.0           | 54.1                   | 43.1           |\n",
      "| Reasoning       | ARC Challenge (0-shot)       | 83.4           | 87.6           | 74.2                | 94.8           | 88.7                   | 83.7           |\n",
      "|                 | GPAO (0-shot, CoT)           | 32.8           | 28.8           | 46.7                | 33.3           | 30.8                   |                |\n",
      "| Tool use        | BFCL                          | 76.1           | 60.4           | 84.8                |                | 85.9                   |                |\n",
      "|                 | Nexus                         | 38.5           | 30.0           | 24.7                | 56.7           | 48.5                   | 37.2           |\n",
      "| Long context    | ZeroSCROLLS/QUALITY          | 81.0           | -              | -                   | 90.5           | -                      |                |\n",
      "|                 | InfiniteBench/In/MC          |  -             | -              | -                   | 78.2           | -                      |                |\n",
      "|                 | NIN/Multi-needle              | 98.8           | -              | -                   | 97.5           | -                      |                |\n",
      "| Multilingual    | Multilingual MGSM (0-shot)   | 68.9           | 53.2           | 29.9                | 86.9           | 71.1                   | 51.4           |\n",
      "\n",
      "## Llama 3.1 405B Human Evaluation\n",
      "\n",
      "| Comparison                               | Win Rate | Tie Rate | Loss Rate |\n",
      "|------------------------------------------|----------|----------|-----------|\n",
      "| Llama 3.1 405B vs GPT-4-0125-Preview    | 23.3%    | 52.2%    | 24.5%     |\n",
      "| Llama 3.1 405B vs GPT-4o                 | 19.1%    | 51.7%    | 29.2%     |\n",
      "| Llama 3.1 405B vs Claude 3.5 Sonnet      | 24.9%    | 50.8%    | 24.2%     |\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b7099cb9ce541d3d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
